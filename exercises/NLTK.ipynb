{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Module 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Text Analysis with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" Dostoevsky was the son of a doctor. \n",
    "His parents were very hard-working and deeply religious people,\n",
    "but so poor that they lived with their five children in only\n",
    "two rooms. The father and mother spent their evenings\n",
    "in reading aloud to their children, generally from books of\n",
    "a serious character.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dostoevsky was the son of a doctor.  His parents were very hard-working and deeply religious people, but so poor that they lived with their five children in only two rooms. The father and mother spent their evenings in reading aloud to their children, generally from books of a serious character.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('corpus/document.txt').read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text.lower().split()))\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.count('their')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict_values(['a', 'b']), dict_values(['a', 'b'])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = [{1:'a', 2:'b'}, {3:'a', 4:'b'}]\n",
    "# [i.values() for i in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_words = [word for word in vocab if len(word) > 5]\n",
    "# long_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dostoevsky was the son of a doctor.',\n",
       " 'His parents were very hard-working and deeply religious people, but so poor that they lived with their five children in only two rooms.',\n",
       " 'The father and mother spent their evenings in reading aloud to their children, generally from books of a serious character.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dostoevsky',\n",
       " 'was',\n",
       " 'the',\n",
       " 'son',\n",
       " 'of',\n",
       " 'a',\n",
       " 'doctor',\n",
       " '.',\n",
       " 'His',\n",
       " 'parents',\n",
       " 'were',\n",
       " 'very',\n",
       " 'hard-working',\n",
       " 'and',\n",
       " 'deeply',\n",
       " 'religious',\n",
       " 'people',\n",
       " ',',\n",
       " 'but',\n",
       " 'so',\n",
       " 'poor',\n",
       " 'that',\n",
       " 'they',\n",
       " 'lived',\n",
       " 'with',\n",
       " 'their',\n",
       " 'five',\n",
       " 'children',\n",
       " 'in',\n",
       " 'only',\n",
       " 'two',\n",
       " 'rooms',\n",
       " '.',\n",
       " 'The',\n",
       " 'father',\n",
       " 'and',\n",
       " 'mother',\n",
       " 'spent',\n",
       " 'their',\n",
       " 'evenings',\n",
       " 'in',\n",
       " 'reading',\n",
       " 'aloud',\n",
       " 'to',\n",
       " 'their',\n",
       " 'children',\n",
       " ',',\n",
       " 'generally',\n",
       " 'from',\n",
       " 'books',\n",
       " 'of',\n",
       " 'a',\n",
       " 'serious',\n",
       " 'character',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Tokenize the Web Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "# import urllib.request\n",
    "\n",
    "# url=\"https://en.wikipedia.org/wiki/George_Washington\"\n",
    "url=\"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "text = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176967"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hell'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or online at www.gutenberg.org\\r\\n\\r\\n\\r\\nTitle: Crime and Punishment\\r\\n\\r\\nAuthor: Fyodor Dostoevsky\\r\\n\\r\\nRelease Date: March 28, 2006 [EBook #2554]\\r\\nLast Updated: October 27, 2016\\r\\n\\r\\nLanguage: English\\r\\n\\r\\nCharacter set encoding: UTF-8\\r\\n\\r\\n*** START OF THIS PROJECT GUTENBERG EBOOK CRIME AND PUNISHMENT ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nProduced by John Bickers; and Dagny\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCRIME AND PUNISHMENT\\r\\n\\r\\nBy Fyodor Dostoevsky\\r\\n\\r\\n\\r\\n\\r\\nTranslated By Constance Garnett\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTRANSLATOR’S PREFACE\\r\\n\\r\\nA few words about Dostoevsky himself may help the English reader to\\r\\nunderstand his work.\\r\\n\\r\\nDostoevsky was the son of a doctor. His parents were very hard-working\\r\\nand deeply religious people, but so poor that they lived with their '"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12059"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.',\n",
       " 'You may copy it, give it away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or online at www.gutenberg.org\\r\\n\\r\\n\\r\\nTitle: Crime and Punishment\\r\\n\\r\\nAuthor: Fyodor Dostoevsky\\r\\n\\r\\nRelease Date: March 28, 2006 [EBook #2554]\\r\\nLast Updated: October 27, 2016\\r\\n\\r\\nLanguage: English\\r\\n\\r\\nCharacter set encoding: UTF-8\\r\\n\\r\\n*** START OF THIS PROJECT GUTENBERG EBOOK CRIME AND PUNISHMENT ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nProduced by John Bickers; and Dagny\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCRIME AND PUNISHMENT\\r\\n\\r\\nBy Fyodor Dostoevsky\\r\\n\\r\\n\\r\\n\\r\\nTranslated By Constance Garnett\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTRANSLATOR’S PREFACE\\r\\n\\r\\nA few words about Dostoevsky himself may help the English reader to\\r\\nunderstand his work.',\n",
       " 'Dostoevsky was the son of a doctor.',\n",
       " 'His parents were very hard-working\\r\\nand deeply religious people, but so poor that they lived with their five\\r\\nchildren in only two rooms.',\n",
       " 'The father and mother spent their evenings\\r\\nin reading aloud to their children, generally from books of a serious\\r\\ncharacter.',\n",
       " 'Though always sickly and delicate Dostoevsky came out third in the\\r\\nfinal examination of the Petersburg school of Engineering.',\n",
       " 'There he had\\r\\nalready begun his first work, “Poor Folk.”\\r\\n\\r\\nThis story was published by the poet Nekrassov in his review and\\r\\nwas received with acclamations.',\n",
       " 'The shy, unknown youth found himself\\r\\ninstantly something of a celebrity.',\n",
       " 'A brilliant and successful career\\r\\nseemed to open before him, but those hopes were soon dashed.',\n",
       " 'In 1849 he\\r\\nwas arrested.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257727"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = [1, 2, 3]\n",
    "# 4 not in a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('corpus/document.txt').read()\n",
    "stop_words = ['.',',','a','they','the','his']\n",
    "# stop_words = ['.',',','a','they','the','his','so','and','were','from','that','of','in','only','with','to']\n",
    "tokens = word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [word for word in tokens if word not in stop_words]\n",
    "# filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "# nltk_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'of' in nltk_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('corpus/document.txt').read()\n",
    "tokens = word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "filtered = [word for word in tokens if word not in stop_words]\n",
    "# filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "module"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation\n",
    "type(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "filtered = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "# filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True or False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# type(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(1, 2), match='d'>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('d', 'cdabcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('^abc', 'abcde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(3, 4), match='b'>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('[abc]', 'egtbaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 1), match='A'>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('[a-zA-Z]', 'Aegtbaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 4), match='Aaaa'>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('A[a]*', 'Aaaab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 5), match='Abbbb'>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('Ab{4}', 'Abbbbbbbnc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 2), match='de'>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('^de', 'dear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'dear'[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(3, 4), match='s'>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('s$', 'cars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match('^[a-z]+$', 'abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dostoevsky', 'doctor', 'deeply']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = [word for word in tokens if re.search('^d', word)]\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['was', 'his', 'parents', 'religious', 'rooms', 'evenings', 'books', 'serious']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = [word for word in tokens if re.search('s$', word)]\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = [word for word in tokens if re.match('^[a-z]+$', word)]\n",
    "# text2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Filter Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('corpus/document.txt').read()\n",
    "filtered  = [word for word in word_tokenize(text.lower()) \\\n",
    "             if re.match('^[a-z]+$', word) and word not in stop_words ]\n",
    "# filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ok(token):\n",
    "    return re.match('^[a-z]+$', token) and token not in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_ok('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_sre.SRE_Match"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(re.match('^[a-z]+$', 'abc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(re.match('^[a-z]+$', '.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match('^[a-z]+$', 'ab') and True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered  = [word for word in word_tokenize(text.lower()) if is_ok(word)]\n",
    "# filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# stemmer = LancasterStemmer()\n",
    "stemmer.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'run']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sucess Cases\n",
    "text = \"running run\"\n",
    "[stemmer.stem(w) for w in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean', 'mean']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Failed Cases\n",
    "text =\"meanness meaning\"\n",
    "[stemmer.stem(w) for w in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['running', 'run']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"running runs\"\n",
    "[lemmatizer.lemmatize(w) for w in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meanness', 'meaning']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =\"meanness meaning\"\n",
    "[lemmatizer.lemmatize(w) for w in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('corpus/document.txt').read() # get a string\n",
    "words = word_tokenize(text) # get a list of words\n",
    "tagged = nltk.pos_tag(words)\n",
    "# tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.pos_tag(['meaning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex: POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('corpus/document.txt').read()\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ok(token):\n",
    "    return re.match('^[a-z]+$', token) and token not in stop_words\n",
    "\n",
    "filtered = [word for word in tokens if is_ok(word)]\n",
    "# filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('was', 'VBD'), ('son', 'NN'), ('of', 'IN'), ('doctor', 'NN'), ('parents', 'NNS'), ('were', 'VBD'), ('very', 'RB'), ('and', 'CC'), ('deeply', 'RB'), ('religious', 'JJ'), ('people', 'NNS'), ('but', 'CC'), ('so', 'RB'), ('poor', 'JJ'), ('that', 'IN'), ('lived', 'VBD'), ('with', 'IN'), ('their', 'PRP$'), ('five', 'CD'), ('children', 'NNS'), ('in', 'IN'), ('only', 'RB'), ('two', 'CD'), ('rooms', 'NNS'), ('father', 'RBR'), ('and', 'CC'), ('mother', 'RBR'), ('spent', 'VBD'), ('their', 'PRP$'), ('evenings', 'NNS'), ('in', 'IN'), ('reading', 'VBG'), ('aloud', 'NN'), ('to', 'TO'), ('their', 'PRP$'), ('children', 'NNS'), ('generally', 'RB'), ('from', 'IN'), ('books', 'NNS'), ('of', 'IN'), ('serious', 'JJ'), ('character', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(filtered)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Tagging on Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_words = brown.words()\n",
    "brown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_tagged_words = brown.tagged_words()\n",
    "# brown_tagged_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brown_sents = brown.sents(categories='news')\n",
    "brown_sents = brown.sents(categories='reviews')\n",
    "# brown_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "# brown_tagged_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chucking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('white', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " ('fight', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('black', 'JJ'),\n",
       " ('cat', 'NN')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import chunk\n",
    "\n",
    "text = \"The white dog fight with a black cat\"\n",
    "words = word_tokenize(text)\n",
    "tagged = nltk.pos_tag(words)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_tkinter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree_to_treesegment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \"\"\"\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtkinter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIntVar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMenu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0min_idle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(self, fullname)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMovedModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__loader__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36m_resolve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_import_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36m_import_module\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_import_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;34m\"\"\"Import module, returning the module after the last dot.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/tkinter/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0m_tkinter\u001b[0m \u001b[0;31m# If this fails your Python may not be configured for Tk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mTclError\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTclError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_tkinter'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('NP', [('The', 'DT'), ('white', 'JJ'), ('dog', 'NN')]), Tree('NP', [('fight', 'NN')]), ('with', 'IN'), Tree('NP', [('a', 'DT'), ('black', 'JJ'), ('cat', 'NN')])])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tagged)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gutenberg Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "# gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileid = 'austen-emma.txt'\n",
    "text = gutenberg.raw(fileid)\n",
    "# text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Num of chars : 887071\n",
      " Num of words : 192427\n",
      " Num of sentences : 7752\n"
     ]
    }
   ],
   "source": [
    "print(\" Num of chars :\",len(gutenberg.raw(fileid)))\n",
    "print(\" Num of words :\",len(gutenberg.words(fileid)))\n",
    "print(\" Num of sentences :\",len(gutenberg.sents(fileid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Gutenberg Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = gutenberg.raw(\"austen-sense.txt\")\n",
    "# text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Sense', 'and', 'Sensibility', 'by']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token = sent_tokenize(text)\n",
    "token = word_tokenize(text)\n",
    "token[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Sense', 'and', 'Sensibility', 'by']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.words(\"austen-sense.txt\")[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = brown.raw(categories='news')\n",
    "# text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Num of chars : 883606\n",
      " Num of words : 100554\n",
      " Num of sentences : 4623\n"
     ]
    }
   ],
   "source": [
    "print(\" Num of chars :\",len(brown.raw(categories='news')))\n",
    "print(\" Num of words :\",len(brown.words(categories='news')))\n",
    "print(\" Num of sentences :\",len(brown.sents(categories='news')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reuters Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "# reuters.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuters.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"FRENCH FREE MARKET CEREAL EXPORT BIDS DETAILED\\n  French operators have requested licences\\n  to export 675,500 tonnes of maize, 245,000 tonnes of barley,\\n  22,000 tonnes of soft bread wheat and 20,000 tonnes of feed\\n  wheat at today's European Community tender, traders said.\\n      Rebates requested ranged from 127.75 to 132.50 European\\n  Currency Units a tonne for maize, 136.00 to 141.00 Ecus a tonne\\n  for barley and 134.25 to 141.81 Ecus for bread wheat, while\\n  rebates requested for feed wheat were 137.65 Ecus, they said.\\n  \\n\\n\""
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileid = 'training/9865'\n",
    "\n",
    "text = reuters.raw(fileid)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Num of chars : 533\n",
      " Num of words : 114\n",
      " Num of sentences : 2\n"
     ]
    }
   ],
   "source": [
    "print(\" Num of chars :\",len(reuters.raw(fileid)))\n",
    "print(\" Num of words :\",len(reuters.words(fileid)))\n",
    "print(\" Num of sentences :\",len(reuters.sents(fileid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['barley', 'corn', 'grain', 'wheat']"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.categories(fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie Review Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "# movie_reviews.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileid = 'pos/cv971_10874.txt'\n",
    "text = movie_reviews.raw(fileid)\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Num of chars : 3597\n",
      " Num of words : 724\n",
      " Num of sentences : 28\n"
     ]
    }
   ],
   "source": [
    "print(\" Num of chars :\",len(movie_reviews.raw(fileid)))\n",
    "print(\" Num of words :\",len(movie_reviews.words(fileid)))\n",
    "print(\" Num of sentences :\",len(movie_reviews.sents(fileid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos']"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.categories(fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Own Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['document.txt', 'file1.txt', 'file2.txt', 'jokes.csv']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = 'corpus' \n",
    "my_corpus = PlaintextCorpusReader(corpus_root, '.*')\n",
    "my_corpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dostoevsky was the son of a doctor. \\r\\nHis parents were very hard-working and deeply religious people,\\r\\nbut so poor that they lived with their five children in only\\r\\ntwo rooms. The father and mother spent their evenings\\r\\nin reading aloud to their children, generally from books of\\r\\na serious character.'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileid = 'file1.txt'\n",
    "text = my_corpus.raw(fileid)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Num of chars : 301\n",
      " Num of words : 57\n",
      " Num of sentences : 3\n"
     ]
    }
   ],
   "source": [
    "print(\" Num of chars :\",len(my_corpus.raw(fileid)))\n",
    "print(\" Num of words :\",len(my_corpus.words(fileid)))\n",
    "print(\" Num of sentences :\",len(my_corpus.sents(fileid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dostoevsky was the son of a doctor. \\r\\nHis parents were very hard-working and deeply religious people,\\r\\nbut so poor that they lived with their five children in only\\r\\ntwo rooms. The father and mother spent their evenings\\r\\nin reading aloud to their children, generally from books of\\r\\na serious character.'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_corpus.raw(fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex: Create Your Own Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at', 'one', 'point', 'during', 'brian', 'de', ...]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileid = 'pos/cv971_10874.txt'\n",
    "text = movie_reviews.words(fileid)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = nltk.FreqDist(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 35), ('.', 27), ('the', 24), ('a', 22), (\"'\", 21)]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distr.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distr[\"stone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def is_ok(token):\n",
    "    return re.match('^[a-z]+$', token) and token not in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word.lower() for word in text if is_ok(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 5), ('de', 5), ('palma', 5), ('scarface', 4), ('stone', 4)]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distr.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex: Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileid = 'training/9865'\n",
    "text = reuters.words(fileid)\n",
    "words = [word.lower() for word in text if is_ok(word)]\n",
    "distr = nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tonnes', 4), ('wheat', 4), ('requested', 3), ('maize', 2), ('barley', 2)]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distr.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distr[\"maize\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['female.txt', 'male.txt']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "names.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names.words('male.txt')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3] + [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_names = [(name, 'male') for name in names.words('male.txt')] \\\n",
    "                + [(name, 'female') for name in names.words('female.txt')]\n",
    "# labeled_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7944"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fernandina', 'female'),\n",
       " ('Emmaline', 'female'),\n",
       " ('Chicky', 'female'),\n",
       " ('Kynthia', 'female'),\n",
       " ('Antoni', 'male'),\n",
       " ('Fifine', 'female'),\n",
       " ('Leo', 'male'),\n",
       " ('Dru', 'male'),\n",
       " ('Marmaduke', 'male'),\n",
       " ('Gerda', 'female')]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(labeled_names)\n",
    "labeled_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Extract last letter of a name as the feature and form feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = {1:'123', 2: '456'}\n",
    "# a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(name):\n",
    "    return {'last_letter': name[-1], 'first_letter': name[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'a', 'first_letter': 'F'}"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor('Fernandina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = [(feature_extractor(name), gender) \\\n",
    "              for (name, gender) in labeled_names]\n",
    "# featureset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7944"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featureset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Split the feature set to training/testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = featureset[500:], featureset[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4# Step 4/5: Load the classifier and perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Prediction/Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(feature_extractor('Neo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.602"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex: Supervied Text Classification Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews.fileids('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.words('neg/cv000_29416.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Data \n",
    "documents = [(movie_reviews.words(fileid), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileid = 'neg/cv000_29416.txt'\n",
    "fileid.split('/')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews.fileids()\n",
    "documents = [(movie_reviews.words(fileid), fileid.split('/')[0])\n",
    "             for fileid in movie_reviews.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (1, 2)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract Feature\n",
    "# all_words = []\n",
    "# for w in movie_reviews.words():\n",
    "#     all_words.append(w.lower())\n",
    "\n",
    "all_words = [w.lower() for w in movie_reviews.words()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords and punctuations\n",
    "all_words  = [word for word in all_words if is_ok(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)\n",
    "# all_words.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = list(all_words.keys())[:3000]\n",
    "# common_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'abc', 2: 'dfg'}"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {}\n",
    "a[1] = 'abc'\n",
    "a[2] = 'dfg'\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(review_words):\n",
    "    review_words = set(review_words)\n",
    "    features = {}\n",
    "    for common_word in common_words:\n",
    "        features[common_word] = (common_word in review_words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_extractor(['plot', 'one'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = [(feature_extractor(review_words), sentiment) \\\n",
    "              for (review_words, sentiment) in documents]\n",
    "# featureset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split the feature set to training/testing datasets\n",
    "training_set, testing_set = featureset[:1900],featureset[1900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4/5: Load the classifier and perform training\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Prediction/Evaluation\n",
    "nltk.classify.accuracy(classifier, testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"naivebayes.pickle\",\"wb\")\n",
    "pickle.dump(classifier, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"naivebayes.pickle\", \"rb\")\n",
    "classifier = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(feature_extractor('atrocious'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Text Classification with Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Data \n",
    "from nltk.corpus import names\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')]\\\n",
    "        + [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "random.shuffle(labeled_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract last letter of a name as the feature\n",
    "def feature_extractor(name):\n",
    "    return {'last_letter': name[-1]}\n",
    "\n",
    "featureset = [(feature_extractor(name), gender) for (name, gender) in labeled_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split the feature set to training/testing datasets\n",
    "train_set, test_set = featureset[500:], featureset[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))>"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4/5: Load the classifier and perform training\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = SklearnClassifier(MultinomialNB())\n",
    "classifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# classifier = SklearnClassifier(LogisticRegression())\n",
    "# classifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Prediction/Evaluation\n",
    "classifier.classify(feature_extractor('Neo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.762"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex: Supervised Text Classification with Scikit Learn Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Data \n",
    "documents = [(movie_reviews.words(fileid), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract Feature\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords and punctuations\n",
    "all_words  = [word for word in all_words if is_ok(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = list(all_words.keys())[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(review_words):\n",
    "    review_words = set(review_words)\n",
    "    features = {}\n",
    "    for common_word in common_words:\n",
    "        features[common_word] = (common_word in review_words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = [(feature_extractor(review_words), sentiment) for (review_words, sentiment) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split the feature set to training/testing datasets\n",
    "training_set, testing_set = featureset[:1900],featureset[1900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))>"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4/5: Load the classifier and perform training\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = SklearnClassifier(LogisticRegression())\n",
    "\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# classifier = SklearnClassifier(MultinomialNB())\n",
    "\n",
    "classifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Prediction/Evaluation\n",
    "classifier.classify(feature_extractor('atrocious'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dostoevsky was the son of a doctor.  His parents were very hard-working and deeply religious people, but so poor that they lived with their five children in only two rooms. The father and mother spent their evenings in reading aloud to their children, generally from books of a serious character.'"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('corpus/document.txt').read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = bigrams(word_tokenize(text))\n",
    "# list(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex: Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ok(token):\n",
    "    return re.match('^[a-z]+$', token) and token not in stop_words\n",
    "\n",
    "filtered = [word for word in word_tokenize(text.lower()) if is_ok(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import trigrams\n",
    "ngrams = trigrams(filtered)\n",
    "# list(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gutenberg Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import gutenberg\n",
    "# gutenberg.sents()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Word2Vec(gutenberg.sents(),min_count=1, \n",
    "                     window=5, size=32, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Word2Vec(gutenberg.sents(),min_count=1, \n",
    "                     window=5, size=32, sg = 0)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anqitu/.pyenv/versions/3.6.8/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.9649154 ,  0.18507943, -3.2940607 , -1.7971218 ,  0.6372929 ,\n",
       "       -2.2874842 , -2.5008888 ,  0.78844815,  2.6457336 , -2.0863624 ,\n",
       "        0.24633124, -0.05431908, -5.6316857 , -1.5423057 , -1.5180963 ,\n",
       "       -1.204056  , -2.8001924 , -1.8601631 , -3.133689  , -4.3746805 ,\n",
       "       -0.42369446,  2.7642775 ,  1.5918746 , -0.9573553 , -0.2375301 ,\n",
       "       -3.0730343 ,  0.62505186, -4.3279815 , -0.24483997, -1.9834069 ,\n",
       "       -3.1793523 , -3.4835222 ], dtype=float32)"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding['man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anqitu/.pyenv/versions/3.6.8/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('body', 0.7932837009429932),\n",
       " ('woman', 0.7869760990142822),\n",
       " ('person', 0.7820342779159546),\n",
       " ('child', 0.758833110332489),\n",
       " ('stranger', 0.7572041749954224)]"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.most_similar('man', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anqitu/.pyenv/versions/3.6.8/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('child', 0.850990891456604),\n",
       " ('gentleman', 0.8423829078674316),\n",
       " ('lady', 0.8334667086601257),\n",
       " ('person', 0.7946107387542725),\n",
       " ('man', 0.7869760990142822)]"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.most_similar('woman', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anqitu/.pyenv/versions/3.6.8/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('prophet', 0.7435209155082703),\n",
       " ('son', 0.7320716381072998),\n",
       " ('Joshua', 0.7212377786636353),\n",
       " ('David', 0.7148489952087402),\n",
       " ('Joseph', 0.7099349498748779),\n",
       " ('captain', 0.6978978514671326),\n",
       " ('Moses', 0.691023051738739),\n",
       " ('Solomon', 0.6904488205909729),\n",
       " ('Jacob', 0.6657432317733765),\n",
       " ('Jesus', 0.6606374979019165)]"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.save('gutenberg_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Word2Vec.load('gutenberg_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anqitu/.pyenv/versions/3.6.8/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('body', 0.7932837009429932),\n",
       " ('woman', 0.7869760990142822),\n",
       " ('person', 0.7820342779159546),\n",
       " ('child', 0.758833110332489),\n",
       " ('stranger', 0.7572041749954224)]"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.most_similar('man', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex: Word2Vec for Movie Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = Word2Vec(movie_reviews.sents(),min_count=1,size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anqitu/.pyenv/versions/3.6.8/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.9845081567764282),\n",
       " ('girl', 0.9517439603805542),\n",
       " ('renounce', 0.9256883859634399),\n",
       " ('son', 0.9180031418800354),\n",
       " ('overracting', 0.9086120128631592)]"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.most_similar('man', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anqitu/.pyenv/versions/3.6.8/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('man', 0.9845081567764282),\n",
       " ('girl', 0.9522714614868164),\n",
       " ('child', 0.9326740503311157),\n",
       " ('nuzzly', 0.9323827028274536),\n",
       " ('son', 0.9285199046134949)]"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.most_similar('woman', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = \"GoogleNews-vectors-negative300.bin\"\n",
    "embedding = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.32617188,  0.13085938,  0.03466797, -0.08300781,  0.08984375,\n",
       "       -0.04125977, -0.19824219,  0.00689697,  0.14355469,  0.0019455 ,\n",
       "        0.02880859, -0.25      , -0.08398438, -0.15136719, -0.10205078,\n",
       "        0.04077148, -0.09765625,  0.05932617,  0.02978516, -0.10058594,\n",
       "       -0.13085938,  0.001297  ,  0.02612305, -0.27148438,  0.06396484,\n",
       "       -0.19140625, -0.078125  ,  0.25976562,  0.375     , -0.04541016,\n",
       "        0.16210938,  0.13671875, -0.06396484, -0.02062988, -0.09667969,\n",
       "        0.25390625,  0.24804688, -0.12695312,  0.07177734,  0.3203125 ,\n",
       "        0.03149414, -0.03857422,  0.21191406, -0.00811768,  0.22265625,\n",
       "       -0.13476562, -0.07617188,  0.01049805, -0.05175781,  0.03808594,\n",
       "       -0.13378906,  0.125     ,  0.0559082 , -0.18261719,  0.08154297,\n",
       "       -0.08447266, -0.07763672, -0.04345703,  0.08105469, -0.01092529,\n",
       "        0.17480469,  0.30664062, -0.04321289, -0.01416016,  0.09082031,\n",
       "       -0.00927734, -0.03442383, -0.11523438,  0.12451172, -0.0246582 ,\n",
       "        0.08544922,  0.14355469, -0.27734375,  0.03662109, -0.11035156,\n",
       "        0.13085938, -0.01721191, -0.08056641, -0.00708008, -0.02954102,\n",
       "        0.30078125, -0.09033203,  0.03149414, -0.18652344, -0.11181641,\n",
       "        0.10253906, -0.25976562, -0.02209473,  0.16796875, -0.05322266,\n",
       "       -0.14550781, -0.01049805, -0.03039551, -0.03857422,  0.11523438,\n",
       "       -0.0062561 , -0.13964844,  0.08007812,  0.06103516, -0.15332031,\n",
       "       -0.11132812, -0.14160156,  0.19824219, -0.06933594,  0.29296875,\n",
       "       -0.16015625,  0.20898438,  0.00041771,  0.01831055, -0.20214844,\n",
       "        0.04760742,  0.05810547, -0.0123291 , -0.01989746, -0.00364685,\n",
       "       -0.0135498 , -0.08251953, -0.03149414,  0.00717163,  0.20117188,\n",
       "        0.08300781, -0.0480957 , -0.26367188, -0.09667969, -0.22558594,\n",
       "       -0.09667969,  0.06494141, -0.02502441,  0.08496094,  0.03198242,\n",
       "       -0.07568359, -0.25390625, -0.11669922, -0.01446533, -0.16015625,\n",
       "       -0.00701904, -0.05712891,  0.02807617, -0.09179688,  0.25195312,\n",
       "        0.24121094,  0.06640625,  0.12988281,  0.17089844, -0.13671875,\n",
       "        0.1875    , -0.10009766, -0.04199219, -0.12011719,  0.00524902,\n",
       "        0.15625   , -0.203125  , -0.07128906, -0.06103516,  0.01635742,\n",
       "        0.18261719,  0.03588867, -0.04248047,  0.16796875, -0.15039062,\n",
       "       -0.16992188,  0.01831055,  0.27734375, -0.01269531, -0.0390625 ,\n",
       "       -0.15429688,  0.18457031, -0.07910156,  0.09033203, -0.02709961,\n",
       "        0.08251953,  0.06738281, -0.16113281, -0.19628906, -0.15234375,\n",
       "       -0.04711914,  0.04760742,  0.05908203, -0.16894531, -0.14941406,\n",
       "        0.12988281,  0.04321289,  0.02624512, -0.1796875 , -0.19628906,\n",
       "        0.06445312,  0.08935547,  0.1640625 , -0.03808594, -0.09814453,\n",
       "       -0.01483154,  0.1875    ,  0.12792969,  0.22753906,  0.01818848,\n",
       "       -0.07958984, -0.11376953, -0.06933594, -0.15527344, -0.08105469,\n",
       "       -0.09277344, -0.11328125, -0.15136719, -0.08007812, -0.05126953,\n",
       "       -0.15332031,  0.11669922,  0.06835938,  0.0324707 , -0.33984375,\n",
       "       -0.08154297, -0.08349609,  0.04003906,  0.04907227, -0.24121094,\n",
       "       -0.13476562, -0.05932617,  0.12158203, -0.34179688,  0.16503906,\n",
       "        0.06176758, -0.18164062,  0.20117188, -0.07714844,  0.1640625 ,\n",
       "        0.00402832,  0.30273438, -0.10009766, -0.13671875, -0.05957031,\n",
       "        0.0625    , -0.21289062, -0.06542969,  0.1796875 , -0.07763672,\n",
       "       -0.01928711, -0.15039062, -0.00106049,  0.03417969,  0.03344727,\n",
       "        0.19335938,  0.01965332, -0.19921875, -0.10644531,  0.01525879,\n",
       "        0.00927734,  0.01416016, -0.02392578,  0.05883789,  0.02368164,\n",
       "        0.125     ,  0.04760742, -0.05566406,  0.11572266,  0.14746094,\n",
       "        0.1015625 , -0.07128906, -0.07714844, -0.12597656,  0.0291748 ,\n",
       "        0.09521484, -0.12402344, -0.109375  , -0.12890625,  0.16308594,\n",
       "        0.28320312, -0.03149414,  0.12304688, -0.23242188, -0.09375   ,\n",
       "       -0.12988281,  0.0135498 , -0.03881836, -0.08251953,  0.00897217,\n",
       "        0.16308594,  0.10546875, -0.13867188, -0.16503906, -0.03857422,\n",
       "        0.10839844, -0.10498047,  0.06396484,  0.38867188, -0.05981445,\n",
       "       -0.0612793 , -0.10449219, -0.16796875,  0.07177734,  0.13964844,\n",
       "        0.15527344, -0.03125   , -0.20214844, -0.12988281, -0.10058594,\n",
       "       -0.06396484, -0.08349609, -0.30273438, -0.08007812,  0.02099609],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding['man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.7664012908935547),\n",
       " ('boy', 0.6824870109558105),\n",
       " ('teenager', 0.6586930155754089),\n",
       " ('teenage_girl', 0.6147903800010681),\n",
       " ('girl', 0.5921714305877686)]"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.most_similar('man', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.7664012908935547),\n",
       " ('girl', 0.7494640946388245),\n",
       " ('teenage_girl', 0.7336829900741577),\n",
       " ('teenager', 0.631708562374115),\n",
       " ('lady', 0.6288785934448242)]"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.most_similar('woman', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8",
   "language": "python",
   "name": "python3.6.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
